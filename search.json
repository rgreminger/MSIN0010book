[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Analytics with R",
    "section": "",
    "text": "Preface\nThis book is written for use in MSIN0010: Data Analytics I at the UCL School of Management. It is meant to serve as a supplement to lecture and seminar materials and specifically focuses on applications in R.\nLast update: October 2025",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "00R.html",
    "href": "00R.html",
    "title": "Getting Started with R",
    "section": "",
    "text": "Why use R?\nR is a statistical software environment that is widely used by statisticians, social scientists, and data analysts. R is different from “point-and-click” software packages like Microsoft Excel, SPSS, or Tableau in that it requires the user to write code via a command line interface. For this reason, R is also often referred to as a programming language. However, the R environment is much more interactive than other programming languages like C or Java which makes it easier to learn and use.\nHere are a few key advantages of R.\nLastly, learning R is a tangible and highly-valued skill you can put on your CV!",
    "crumbs": [
      "Getting Started with R"
    ]
  },
  {
    "objectID": "00R.html#why-use-r",
    "href": "00R.html#why-use-r",
    "title": "Getting Started with R",
    "section": "",
    "text": "R is free and open source1.\nR is available on Windows, Mac OS, and UNIX/Linux.\nR is flexible: you can write, modify, save, and share your own code.\nR is powerful: you can do everything from making high-quality graphics to running sophisticated statistical machine learning models.\nR is popular: there is a large and growing online community of users making it easy to find answers to any problem you run into.",
    "crumbs": [
      "Getting Started with R"
    ]
  },
  {
    "objectID": "00R.html#what-does-r-do",
    "href": "00R.html#what-does-r-do",
    "title": "Getting Started with R",
    "section": "What does R do?",
    "text": "What does R do?\nAt its core, R (and any other programming language) just translates human-written code into instructions that the computer understands, and then has the computer execute it for us. For example, to assign the number 2 to the variable a we just type\n\na &lt;- 2\n\nR then translates this instruction into code that looks something like this (which is Assembly code):\n\n   pushq   %rbp\n   movq    %rsp, %rbp\n   movl    $1, %eax\n   popq    %rbp\n   retq\n   nopl    (%rax,%rax)\n\nClearly, it is much more convenient to write in R code than in these complicated instructions! Besides, the above will be even further translated into zeros and ones, which would be impossible for us to write instructions in.",
    "crumbs": [
      "Getting Started with R"
    ]
  },
  {
    "objectID": "00R.html#downloading-r-and-rstudio",
    "href": "00R.html#downloading-r-and-rstudio",
    "title": "Getting Started with R",
    "section": "Downloading R and RStudio",
    "text": "Downloading R and RStudio\nTo download R, go to https://cran.rstudio.com/ and choose “Download R for …” your operating system (Windows, Mac, or Linux). For other questions, see the R FAQ.\nAfter downloading R, you should also download RStudio, which is an integrated development enviroment (IDE) for R. Whereas we could use any basic text editor to write code for R, and IDE like Rstudio provides a much more interactive and user-friendly interface for using R. To download it, go here and download the installer for your operating system. Other popular IDEs for R include Positron and VS Code.\nAn alternative to run R locally on your computer is to run it in the cloud. By registering on Posit cloud, you can get the Rstudio experience directly in your browser, without the need to install anything on your computer. However, this can only be used while connected to the internet, and the free account has resource limitations (e.g. RAM).",
    "crumbs": [
      "Getting Started with R"
    ]
  },
  {
    "objectID": "00R.html#installing-and-loading-packages",
    "href": "00R.html#installing-and-loading-packages",
    "title": "Getting Started with R",
    "section": "Installing and Loading Packages",
    "text": "Installing and Loading Packages\nAs you will see, one of the most attractive features of R is its library of over 10,000 packages. R packages – which are collections of R functions, code, and data sets – allow us to use code written by others in order to use certain data sets, make certain graphs, or run certain models.\nFor example, in this course we will discuss a variety of regression models including linear regression models and regression trees. While the R function to estimate a linear regression model (called lm) is included in “base” R, the function to estimate a regression tree is not. Rather than writing the code ourselves, we can download an R package!\nOne package that provides code for estimating regression trees is called rpart. To use functions within the rpart package, we must first install it.\n\ninstall.packages(\"rpart\")\n\nAlternatively, you can navigate to the “Packages” tab in RStudio (likely in the lower right panel), click “Install”, and search for rpart.\nNote: You only need to install a package once! After a package is installed, it will remain installed until you upgrade your version of R/RStudio.\nHowever, in each R session (i.e., each time you open RStudio), you will need to load the package.\n\nlibrary(\"rpart\")\n\nAgain, an alternative is to navigate to the “Packages” tab in RStudio, find the package name, and click on the box to the left of the name.",
    "crumbs": [
      "Getting Started with R"
    ]
  },
  {
    "objectID": "00R.html#footnotes",
    "href": "00R.html#footnotes",
    "title": "Getting Started with R",
    "section": "",
    "text": "https://opensource.org↩︎",
    "crumbs": [
      "Getting Started with R"
    ]
  },
  {
    "objectID": "01analytics.html",
    "href": "01analytics.html",
    "title": "1  Introduction to Data Analytics",
    "section": "",
    "text": "1.1 Loading Data Sets\nIn this chapter, we will get acclimated to working with data using a suite of packages in R called the tidyverse.1 If you are interested in a complete introduction to the tidyverse syntax, see R for Data Science. Specifically, for details on data visualization see Chapter 3 - Data Visualization and Chapter 7 - Exploratory Data Analysis.\nTo use the tidyverse packages, we first need to load them in R. Without running this command first, any of the functions used below that are part of tidyverse will produce an error.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Data Analytics</span>"
    ]
  },
  {
    "objectID": "01analytics.html#loading-data-sets",
    "href": "01analytics.html#loading-data-sets",
    "title": "1  Introduction to Data Analytics",
    "section": "",
    "text": "1.1.1 Data formats\nData sets come in different storage formats:\n\n.csv: Commas separate the values in each row.\n.xls: Excel spreadsheet.\n.txt: Text files\nAs part of R package.\n\nR can read in data from most (if not all) of these formats. In our examples, we will use data set from R packages and .csv files.\n\n\n1.1.2 Reading in Data from R packages\nTo read in data from an R package, we use the data() function. For example, the ISLR provides several data sets. To read in the OJ data, we simply run data(OJ). As the data is part of a package, don’t forget to first load the package.\n\nlibrary(ISLR)\ndata(OJ)\n\n\n\n1.1.3 Reading in Data from .csv files\nTo read in data from .csv files, we will use the read_csv() function, which is provided by one of the packages in tidyverse.\nThe following line of code reads in a data set that contains weekly prices, promotional activity, and sales for 20 different brands of beer. The data set comes from many stores within one Chicago grocery retail chain – Dominick’s Finer Foods – and spans more than five years of transactions. The complete raw data are publically available from the Kilts Center for Marketing at the University of Chicago.2\n\nbeer &lt;- read_csv(\"beer.csv\")",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Data Analytics</span>"
    ]
  },
  {
    "objectID": "01analytics.html#the-working-directory",
    "href": "01analytics.html#the-working-directory",
    "title": "1  Introduction to Data Analytics",
    "section": "1.2 The Working Directory",
    "text": "1.2 The Working Directory\nFor above code to work, the data file beer.csv needs to be located in the current working directory of R. This is the directory that R uses as a default to look for files. To see what your current working directory is, we can use the getwd() function, which is short for “get working directory”. The output is the path to the folder that R is currently looking in for files.\n\ngetwd()\n\n\n\n[1] \"C:/Users/Rafael/Documents\"\n\n\n\n1.2.1 Using setwd() to change the working directory\nWe can change the working directory with setwd(\"PATH_TO_NEW_WORKING_DIRECTORY\"), where NEWPATH is the path to the new working directory. Note that we must use / instead of \\ in the path name, otherwise R will not be able to find the folder and give an error.\n\nsetwd(\"C:\\Users\\Rafael\\Documents\")\n\nError: '\\U' used without hex digits in character string (&lt;input&gt;:1:11)\n\n\nExercise 1: Change the working directory to your downloads folder, and use the getwd() function to confirm that the working directory has changed.\n\n\nSolution:\n\n\nsetwd(\"C:/Users/Rafael/Downloads\")\ngetwd()\n\n\n\n[1] \"C:/Users/Rafael/Downloads\"\n\n\n\n\n\n1.2.2 Using the “Files” tab to change the working directory\nAnother way to change the working directory is to use the “Files” tab in the bottom right panel of RStudio. Navigate to the folder you want to use as your working directory, and click “More” and then “Set As Working Directory”.\n\n\n\n1.2.3 Open a script to automatically set the working directory\nAnother way to set the working directory is to open a .R file with RStudio. RStudio will automatically set the working directory to the folder where the .R file is located. This makes it very convenient to keep all of your code and data files in the same folder.\nTo make use of this, you should create a new folder for every project (or assignment). You can do this directly in RStudio by going to the “File” tab, navigating to whereever you want the new folder to be, and then clicking “Create a new folder”. This will open a window that allows you to create a new folder and name it.\n\nThen, you can create a new .R file by going to the “File” tab, clicking “New File”, and then “R Script”. This will allow you to name and save a new script in the folder. Then, whenever you open this script with a new RStudio session, RStudio will automatically set the working directory to the folder where the script is located.\nIMPORTANT: This only works if all instances of RStudio have been closed! If you have another RStudio session open, it will not automatically set the working directory.\n\n\n\n1.2.4 Providing full file paths to read in data\nInstead of changing the working directory, we can also specify the full file path to the beer.csv data we want to load. We can do this by running read_csv(\"PATH/beer.csv\"), where we replace PATH with the path to the beer.csv file. Note again that we need to use / and not \\ in the file path.\nbeer &lt;- read_csv(\"C:/Users/Rafael/Documents/beer.csv\")\n\n\n1.2.5 Uploading data in RStudio/Posit cloud\nIf you are using RStudio in the cloud, you need to upload the .csv data file to the cloud before you can read it in. To do this, click on the “Upload” button in the “Files” tab, and then select the file you want to upload using the file browser.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Data Analytics</span>"
    ]
  },
  {
    "objectID": "01analytics.html#inspecting-data",
    "href": "01analytics.html#inspecting-data",
    "title": "1  Introduction to Data Analytics",
    "section": "1.3 Inspecting Data",
    "text": "1.3 Inspecting Data\nWe always want to view the data after importing to make sure all the values were read-in correctly. To inspect the first few lines of a data set, use the head( ) command.\n\nhead(beer,3)\n\n\n\n\n\n\n\nstore\nupc\nweek\nmove\nprice\nsale\nprofit\nbrand\npacksize\nitemsize\nunits\n\n\n\n\n86\n1820000016\n91\n23\n3.49\nNA\n19.05\nBUDWEISER BEER\n6\n12\noz\n\n\n86\n1820000784\n91\n9\n3.79\nNA\n28.23\nO'DOUL'S N/A LONGNEC\n6\n12\noz\n\n\n86\n1820000834\n91\n9\n3.69\nNA\n22.03\nBUDWEISER BEER LONG\n6\n12\noz\n\n\n86\n1820000987\n91\n78\n3.29\nB\n5.78\nMICHELOB REGULAR BEE\n6\n12\noz\n\n\n86\n3410000354\n91\n35\n3.69\nNA\n22.98\nMILLER LITE BEER\n6\n12\noz\n\n\n86\n3410000554\n91\n12\n3.69\nNA\n22.98\nMILLER GENUINE DRAFT\n6\n12\noz\n\n\n\n\n\n\n\nWe can see that our data set contains 11 different variables (i.e., columns). A brief summary of each variable is provided below.\n\nstore: unique store ID number\nupc: Universal Product Code\nweek: week ID number\nmove: number of units sold\nprice: retail price in US dollars\nsale: indicator of promotional activity\nprofit: gross profit margin\nbrand: brand name\npacksize: number of items in one package\nitemsize: size of items in one package\nunits: units of items",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Data Analytics</span>"
    ]
  },
  {
    "objectID": "01analytics.html#summary-statistics",
    "href": "01analytics.html#summary-statistics",
    "title": "1  Introduction to Data Analytics",
    "section": "1.4 Summary Statistics",
    "text": "1.4 Summary Statistics\nWe can compute summary statistics in the tidyverse by combining the summarise operator with any one (or many!) of R’s built-in statistics functions. A few of the most common are listed below.\n\n\n\n\n\nStatistic\nR Function\n\n\n\n\nmean\n`mean( )`\n\n\nmedian\n`median( )`\n\n\nvariance\n`var( )`\n\n\nstandard deviation\n`sd( )`\n\n\ncorrelation\n`cor( )`\n\n\n\n\n\n\n\nFor example, let’s compute the average price across all products and weeks. For this, we’re going to use the pipe operator %&gt;%, which works by putting together a function from the left to right. Below, the dataset beer is put as an argument into the summarise() function. An alternative way of writing the same function would be summarise(beer,mean(price)).\n\nbeer %&gt;%\n  summarise(mean(price))\n\n# A tibble: 1 × 1\n  `mean(price)`\n          &lt;dbl&gt;\n1          4.28\n\n\nNow suppose we wanted to find the average price for only one brand of beer, say Budweiser. To do this, we can use the filter( ) operator to select rows in the data that satisfy certain conditions. Here we want Budweiser beers so the condition is that brand is equal to BUDWEISER BEER, or brand==\"BUDWEISER BEER\". Note that a double equals sign == is always used when writing logical statements to check equality. As above, we again use %&gt;% to put the different commands together.\n\nbeer %&gt;%\n  filter(brand==\"BUDWEISER BEER\") %&gt;%\n  summarise(mean(price))\n\n# A tibble: 1 × 1\n  `mean(price)`\n          &lt;dbl&gt;\n1          3.81\n\n\nTo compute summary statistics for multiple brands, we can use the group_by( ) operator. As the name suggests, this operator tells R to first group by a certain categorical variable, and to compute a summary for each level that the given variable takes on.\n\nbeer %&gt;%\n  group_by(brand) %&gt;%\n  summarise(mean(price))\n\n# A tibble: 19 × 2\n   brand                `mean(price)`\n   &lt;chr&gt;                        &lt;dbl&gt;\n 1 BECK'S REG BEER NR B          5.88\n 2 BERGHOFF REGULAR BEE          3.94\n 3 BUDWEISER BEER                3.81\n 4 BUDWEISER BEER LONG           3.75\n 5 CORONA EXTRA BEER NR          5.80\n 6 HEINEKEN BEER N.R.BT          6.34\n 7 LOWENBRAU BEER NR BT          4.05\n 8 MICHELOB REGULAR BEE          4.04\n 9 MILLER GEN DRFT LNNR          3.69\n10 MILLER GEN DRFT LT L          3.69\n11 MILLER GENUINE DRAFT          3.78\n12 MILLER HIGH LIFE LNN          3.68\n13 MILLER LITE BEER              3.82\n14 MILLER LITE BEER N.R          3.74\n15 MILLER LITE LONGNECK          3.69\n16 MILLER SHARP'S N/A L          3.36\n17 O'DOUL'S N/A LONGNEC          3.78\n18 OLD STYLE BEER                3.68\n19 SAMUEL ADAMS LAGER N          5.41\n\n\nWe can also easily extend the code above to compute multiple summary statistics across groups.\n\nbeer %&gt;%\n  group_by(brand) %&gt;%\n  summarise(mean(price), mean(move))\n\n# A tibble: 19 × 3\n   brand                `mean(price)` `mean(move)`\n   &lt;chr&gt;                        &lt;dbl&gt;        &lt;dbl&gt;\n 1 BECK'S REG BEER NR B          5.88         18.3\n 2 BERGHOFF REGULAR BEE          3.94         15.6\n 3 BUDWEISER BEER                3.81         16.3\n 4 BUDWEISER BEER LONG           3.75         18.2\n 5 CORONA EXTRA BEER NR          5.80         15.4\n 6 HEINEKEN BEER N.R.BT          6.34         16.7\n 7 LOWENBRAU BEER NR BT          4.05         16.9\n 8 MICHELOB REGULAR BEE          4.04         14.2\n 9 MILLER GEN DRFT LNNR          3.69         51.0\n10 MILLER GEN DRFT LT L          3.69         20.1\n11 MILLER GENUINE DRAFT          3.78         16.4\n12 MILLER HIGH LIFE LNN          3.68         14.1\n13 MILLER LITE BEER              3.82         18.1\n14 MILLER LITE BEER N.R          3.74         18.7\n15 MILLER LITE LONGNECK          3.69         38.4\n16 MILLER SHARP'S N/A L          3.36         11.5\n17 O'DOUL'S N/A LONGNEC          3.78         12.0\n18 OLD STYLE BEER                3.68         13.4\n19 SAMUEL ADAMS LAGER N          5.41         20.6",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Data Analytics</span>"
    ]
  },
  {
    "objectID": "01analytics.html#graphical-summaries",
    "href": "01analytics.html#graphical-summaries",
    "title": "1  Introduction to Data Analytics",
    "section": "1.5 Graphical Summaries",
    "text": "1.5 Graphical Summaries\nData visualization is one of the strengths of the tidyverse. A fairly exhaustive list of graph types can be found at https://www.r-graph-gallery.com. For our purposes, we will start with a few of the most commonly used graphs.\n\n\n\n\n\nGraph\nOperator\n\n\n\n\nhistogram\n`geom_histogram( )`\n\n\nbox plot\n`geom_boxplot( )`\n\n\nbar plot\n`geom_bar( )`\n\n\nline plot\n`geom_line( )`\n\n\nscatter plot\n`geom_point( )`\n\n\n\n\n\n\n\nLet’s start be looking at the distribution of weekly prices across all products.\n\nbeer %&gt;%\n  ggplot(aes(x=price)) +\n  geom_histogram() +\n  labs(title=\"Distribution of Weekly Prices\")\n\n\n\n\n\n\n\n\nThis is an example of a histogram. The variable on the x-axis (price) is split into different bins, and the y-axis counts the number of observations that fall withiin each bin.\nWe can also inspect the distribution of a variable like prices using a boxplot.\n\nbeer %&gt;%\n  ggplot(aes(y=price)) +\n  geom_boxplot() +\n  labs(title=\"Distribution of Weekly Prices\")\n\n\n\n\n\n\n\n\nNotice that the focal variable of interest is now on the y-axis. The rectangular box shown in the middle of the plot indicates three key summary statistics: the bottom line is the 25th percentile, the middle line is the 50th percentile (or median), and the top line is the 75th percentile. The vertical line starting around 2.5 and ending around 7 indicates the full range of prices in the data.\nThe figure above shows the distribution of prices across all prodcuts. However, we may want to explore whether the distribution of prices is different across products. This can be done by defining x=brand within the ggplot( ) function.\n\nbeer %&gt;%\n  ggplot(aes(x=brand, y=price)) +\n  geom_boxplot() +\n  labs(title=\"Distribution of Weekly Prices\")\n\n\n\n\n\n\n\n\nWe have succeeded in creating a box plot for each brand, but the brand labels are impossible to read! As easy fix is to rotate the x-axis labels, which can be controlled using the theme() operator. The theme() operator is generally what we use to change plot formatting, such as the size of the axis labels or the position of the title.\n\nbeer %&gt;%\n  ggplot(aes(x=brand, y=price)) +\n  geom_boxplot() +\n  labs(title=\"Distribution of Weekly Prices\") +\n  theme(axis.text.x = element_text(angle=90, hjust=1),\n        plot.title = element_text(hjust=0.5))\n\n\n\n\n\n\n\n\nMuch better!\nNext, let’s explore the variation of prices over time. We can make a time series plot (or line plot) to do this, where we specify group=brand so that R makes a separate line for each brand.\n\nbeer %&gt;%\n  ggplot(aes(x=week, y=price, group=brand)) +\n  geom_line() +\n  labs(title=\"Beer Prices over Time\") +\n  theme(plot.title = element_text(hjust=0.5))\n\n\n\n\n\n\n\n\nThis was a good attempt, but the plot is not especially useful! While we do notice that the prices are changing over time, we can’t identify the products themselves so we don’t know which products are changing more or less than others.\nWe can fix this in a couple ways. The first thing we will try is to simply add color to the plot above, so that we can identify a product by the color of its line.\n\nbeer %&gt;%\n  ggplot(aes(x=week, y=price, group=brand, color=brand)) +\n  geom_line() +\n  labs(title=\"Beer Prices over Time\") +\n  theme(plot.title = element_text(hjust=0.5))\n\n\n\n\n\n\n\n\nThis is better, but it is still hard to identify products because of how much overlap there is in prices. So maybe the best thing to do is create a separate plot for each brand. This can be easily accomplished using facet_wrap( ).\n\nbeer %&gt;%\n  ggplot(aes(x=week,y=price,group=brand,color=brand)) +\n  geom_line(show.legend=FALSE) +\n  labs(title=\"Beer Prices over Time\") +\n  facet_wrap(brand ~ .) +\n  theme(plot.title = element_text(hjust=0.5),\n        strip.text.x = element_text(size=6))\n\n\n\n\n\n\n\n\nNote that we added show.legend=FALSE to geom_line( ) since we no longer need the color to identify products. We also added an option to theme( ) to control the size of the text to ensure that the labels are all legible.\nFinally, let’s explore the relationship between two variables like price and demand.\n\nbeer %&gt;%\n  ggplot(aes(x=price, y=move)) +\n  geom_point() +\n  labs(title=\"Price vs. Demand\") +\n  theme(plot.title = element_text(hjust=0.5))\n\n\n\n\n\n\n\n\nThis figure matches our intuition from economics, which is that as price increases, demand seems to fall. We can even imagine a line going through these points – this line would be a demand curve!\nAs before, it would be interesting to know how the relationship between price and demand changes across products. Let’s apply the same techniques above – adding color and using separate plots – to investigate.\n\nbeer %&gt;%\n  ggplot(aes(x=price, y=move, color=brand)) +\n  geom_point() +\n  labs(title=\"Price vs. Demand\") +\n  theme(plot.title = element_text(hjust=0.5))\n\n\n\n\n\n\n\n\n\nbeer %&gt;%\n  ggplot(aes(x=price, y=move, color=brand)) +\n  geom_point(show.legend=FALSE) +\n  labs(title=\"Price vs. Demand\") +\n  facet_wrap(brand~.,scales=\"free\") +\n  theme(plot.title = element_text(hjust=0.5),\n        strip.text.x = element_text(size=6))\n\n\n\n\n\n\n\n\nThese last two plots indeed show that demand is negatively related to price (as price increases, demand falls) and that the magnitude of this relationship may change across products.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Data Analytics</span>"
    ]
  },
  {
    "objectID": "01analytics.html#footnotes",
    "href": "01analytics.html#footnotes",
    "title": "1  Introduction to Data Analytics",
    "section": "",
    "text": "https://www.tidyverse.org/↩︎\nhttps://www.chicagobooth.edu/research/kilts/datasets/dominicks↩︎",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Data Analytics</span>"
    ]
  },
  {
    "objectID": "02probability.html",
    "href": "02probability.html",
    "title": "2  Computing Probabilities",
    "section": "",
    "text": "2.1 Normal Distribution\nThere are many common families of probability distributions and we have discussed six so far. The discrete distributions include the discrete Uniform, Bernoulli, and Binomial. The continuous distributions include the continuous Uniform, Normal, and t.\nThis chapter provides a set of examples to show you how to compute probabilities from a few of these distributions in R.\nR has four normal distribution functions: dnorm( ), pnorm( ), qnorm( ), and rnorm( ).\nMore information is also accessible in R if you type ?dnorm, ?pnorm, ?qnorm, or ?rnorm.\nTo learn how to use these functions, we’ll start with a few exercises on the standard normal distribution which is a normal distribution with mean 0 and standard deviation of 1. We will then move on to the more general \\(N(\\mu,\\sigma^2)\\) distribution.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Computing Probabilities</span>"
    ]
  },
  {
    "objectID": "02probability.html#normal-distribution",
    "href": "02probability.html#normal-distribution",
    "title": "2  Computing Probabilities",
    "section": "",
    "text": "dnorm(x,mean,sd) probability density function (PDF) - input: x is the value at which you want to evaluate the normal PDF - output: a positive number since the PDF \\(f(x)\\) must be positive - example: evaluate \\(f(x)\\)\npnorm(q,mean,sd) cumulative distribution function (CDF) - input: q is the value for which you want to find the area below/above - output: a probability - example: compute \\(P(X&lt;q)\\)\nqnorm(p,mean,sd) quantile function - input: p is a probability - output: a real number since \\(X\\in(-\\infty,\\infty)\\) - example: find the value \\(q\\) such that \\(P(X&lt;q)=p\\)\nrnorm(n,mean,sd) random number generator - input: n is the number of observations you want to generate - output: a vector of n real numbers - example: generate n independent \\(N(\\mu,\\sigma^2)\\) random variables\n\n\n\n\n2.1.1 Probability Density Function (dnorm)\nWhen \\(X\\) is a continuous random variable, we know that \\(P(X=x)=0\\). Therefore, dnorm( ) does not return a probability, but rather the height of the PDF. Even though the height of the PDF is not a probability, we can still interpret density evaluations as the relatively likelihood of observing a certain value \\(x\\).\nPROBLEM 1: Let \\(X\\sim N(0,1)\\). Is the value \\(x=1\\) or \\(x=-0.5\\) more likely to occur under this normal distribution?\n\n\nSolution:\n\n\ndnorm(1, mean=0, sd=1)\n\n[1] 0.2419707\n\n\n\ndnorm(-0.5, mean=0, sd=1)\n\n[1] 0.3520653\n\n\nThe results show that \\(x=-0.5\\) is more likely, since \\(f(-0.5)&gt;f(1)\\). This should be expected because we know that density function is symmetric and peaks at the mean value which is 0 here. Since \\(x=-0.5\\) is closer to 0 than \\(x=1\\), it should have higher likelihood under \\(N(0,1)\\) distribution.\n\n\n\n2.1.2 Cumulative Distribution Function (pnorm)\nThe pnorm( ) function is useful for evaluating probabilities of the form \\(P(X\\leq x)\\) or \\(P(X \\geq x)\\).\nPROBLEM 2: If \\(X\\sim N(0,1)\\), what is \\(P(X&lt;0)\\)?\n\n\nSolution:\n\n\npnorm(0, mean=0, sd=1)\n\n[1] 0.5\n\n\n\nPROBLEM 3: If \\(X\\sim N(0,1)\\), what is \\(P(X&lt;1)\\)?\n\n\nSolution:\n\n\npnorm(1, mean=0, sd=1)\n\n[1] 0.8413447\n\n\n\nPROBLEM 4: If \\(X\\sim N(0,1)\\), what is \\(P(X&gt;1)\\)?\n\n\nSolution:\n\nWe have two ways of answering this question. First, we can recognize that \\(P(X&gt;1)=1-P(X\\geq 1)\\).\n\n1-pnorm(1, mean=0, sd=1)\n\n[1] 0.1586553\n\n\nA second approach is to use the lower.tail option within the pnorm( ) function. When lower.tail=TRUE then the pnorm( ) function returns the probability to the left of a given number \\(x\\) and if lower.tail=FALSE then pnorm( ) returns the probability to the right of \\(x\\).\n\npnorm(1, mean=0, sd=1, lower.tail=FALSE)\n\n[1] 0.1586553\n\n\n\nPROBLEM 5: If \\(X\\sim N(0,1)\\), what is \\(P(0&lt;X&lt;1)\\)\n\n\nSolution:\n\n\npnorm(1, mean=0, sd=1) - pnorm(0, mean=0, sd=1)\n\n[1] 0.3413447\n\n\n\nOnce we understand how to use the pnorm( ) function to compute standard normal probabilities, extending the function to compute probabilities of any normal distribution is straightforward. All we have to do is change the mean and sd arguments.\nRemember that the normal functions in R call for the standard deviation \\(\\sigma\\), NOT the variance \\(\\sigma^2\\)!\nPROBLEM 6: If \\(X\\sim N(4,9)\\), what is \\(P(X&lt;0)\\)?\n\n\nSolution:\n\n\npnorm(0, mean=4, sd=3)\n\n[1] 0.09121122\n\n\n\nPROBLEM 7: If \\(X\\sim N(2,3)\\), what is \\(P(X&gt;5)\\)?\n\n\nSolution:\n\n\npnorm(5, mean=2, sd=sqrt(3), lower.tail=FALSE)\n\n[1] 0.04163226\n\n\n\n\n\n2.1.3 Quantile Function (qnorm)\nNext, let’s use the qnorm( ) function to find quantiles of the normal distribution.\nPROBLEM 8: If \\(X\\sim N(0,1)\\), find the value \\(q\\) such that \\(P(X&lt;q)=0.05\\).\n\n\nSolution:\n\n\nqnorm(0.05, mean=0, sd=1)\n\n[1] -1.644854\n\n\n\nPROBLEM 9: If \\(X\\sim N(0,1)\\), find the value \\(q\\) such that \\(P(X&gt;q)=0.025\\). That is, \\(q\\) is the value such that 2.5% of the area under the standard normal PDF is to its right.\n\n\nSolution:\n\n\nqnorm(0.025, mean=0, sd=1, lower.tail=FALSE)\n\n[1] 1.959964\n\n\n\nPROBLEM 10: If \\(X\\sim N(-4,2)\\), find the value \\(q\\) such that \\(P(X&gt;q)=0.1\\). That is, \\(q\\) is the value such that 10% of the area under the \\(N(-4,2)\\) PDF is to its right.\n\n\nSolution:\n\n\nqnorm(0.1, mean=-4, sd=sqrt(2), lower.tail=FALSE)\n\n[1] -2.187612\n\n\n\n\n\n2.1.4 Random Number Generator (rnorm)\nFinally, let’s use rnorm( ) to generate random samples of size \\(n\\) from a normal distribution.\nPROBLEM 11: Generate \\(n=20\\) random variables from a standard normal distribution.\n\n\nSolution:\n\n\nx = rnorm(20, mean=0, sd=1)\nx\n\n [1] -1.08578052 -2.51525266  0.50112721 -1.46349046  0.89933666  0.42076457\n [7] -1.53133499 -0.37842473  0.15526855  0.79426753  0.36203937  0.47682469\n[13]  1.05620180  0.96245182 -1.20061008  0.31344427  0.84498530 -0.50238712\n[19] -1.11387644 -0.04362956\n\nhist(x)\n\n\n\n\n\n\n\n\n\nPROBLEM 12: Generate \\(n=100\\) random variables from a \\(N(10,2)\\) distribution.\n\n\nSolution:\n\n\nx = rnorm(100, mean=10, sd=sqrt(2))\nx\n\n  [1]  9.842401  9.405289 10.929030 10.990341  9.995606  9.937003 10.490203\n  [8] 10.058287 10.126005  7.742390  8.562545  8.028502  8.526251 12.251231\n [15] 12.776681 12.073214  9.947080  8.808330  9.165551 10.281743 10.138185\n [22] 11.595435  9.225106 12.241392  8.305476  9.586018  9.037704 11.437506\n [29]  9.690265  8.381058  8.197428 11.551766 10.321125 10.802257 10.741454\n [36] 10.367206 11.089124  9.658220  7.639096 10.252741 11.213879 10.220065\n [43] 12.600090  9.331105  9.405717  9.995660 10.861830  7.630178  8.720515\n [50]  9.718929  8.737404 11.819054  9.184134  9.167343  9.985605 10.467358\n [57]  7.016921 11.966441 10.050908  8.021857  9.986673 10.990380 11.675890\n [64] 11.491785 11.609429 10.126801 10.391337  7.240403 10.486688  8.027451\n [71]  9.153793  9.947466  9.864073  9.830305  9.834425  9.512957  7.433951\n [78] 10.529513  9.008647  7.873238  8.589169  6.652862 11.163475 12.174325\n [85]  9.562299  9.721394 14.426347  9.737223 11.073780 10.193520  8.374574\n [92]  9.403898  7.350218 11.392020  9.132813  9.356953  7.932916 10.487301\n [99] 10.494659  6.736746\n\nhist(x)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Computing Probabilities</span>"
    ]
  },
  {
    "objectID": "02probability.html#bernoulli-and-binomial-distributions",
    "href": "02probability.html#bernoulli-and-binomial-distributions",
    "title": "2  Computing Probabilities",
    "section": "2.2 Bernoulli and Binomial Distributions",
    "text": "2.2 Bernoulli and Binomial Distributions\nThe Bernoulli and Binomial distributions are intimately related: a Binomial random variable corresponds to the number of successes in \\(n\\) independent Bernoulli trials. For example, consider flipping a coin. Each coin flip can be modelled as a Bernoulli\\((p)\\) random variable with probability of success (heads) equal to \\(p\\). If you flipped a coin \\(n=10\\) times and wanted to model the number of sucesses (heads) in \\(n=10\\) trials, that would be a Binomial(\\(n,p\\)) random variable.\nR has four functions that can be used to compute both Bernoulli and Binomial probabilities: dbinom( ), pbinom( ), qbinom( ), rbinom( ).\n\ndbinom(x,size,prob) probability mass function (PMF) - input: x is the number of successes, size is the number of trials \\(n\\), prob is the probability of success \\(p\\) - output: a probability since \\(0\\leq P(X=x)\\leq1\\) - example: evaluate \\(P(X=x)\\)\npbinom(q,size,prob) probability distribution function (CDF) - input: q is the value for which you want to find the area below/above, size is the number of trials \\(n\\), prob is the probability of success \\(p\\) - output: a probability - example: evaluate \\(P(X\\leq x)\\)\nqbinom(p,size,prob) quantile function\n- input: p is a probability, size is the number of trials \\(n\\), prob is the probability of success \\(p\\) - output: a positive integer since \\(X\\in\\{0,1,\\dotsc,n\\}\\) - example: find \\(q\\) s.t. \\(P(X\\leq q)=p\\)\nrbinom(n,size,prob) random number generator\n- input: n is the number of observations you want to generate, size is the number of trials \\(n\\), prob is the probability of success \\(p\\) - output: a vector of n positive integers - example: generate \\(n\\) independent Binomial\\((n,p)\\) random variables\n\nNote: These functions correspond to the Bernoulli distribution whenever size=1.\nMore information is also accessible in R if you type ?dbinom, ?pbinom, ?qbinom, or ?rbinom.\n\n2.2.1 Probability Mass Function (dbinom)\nPROBLEM 13: If you flip a coin \\(n=5\\) times and in each flip the probability of heads is \\(p=0.5\\), what is the chance that you get 2 successes?\n\n\nSolution:\n\nHere, our random variable \\(X\\) is the number of successes in \\(n\\) independent trials, so \\(X\\sim\\text{Binomial}(n,p)\\) with \\(n=5\\) and \\(p=0.5\\).\n\ndbinom(2, size=5, prob=0.5)\n\n[1] 0.3125\n\n\nWe can also check our answer using the Binomial probability mass function: \\(P(X=x)={n\\choose x}p^x(1-p)^{n-x}\\).\n\nchoose(5,2)*0.5^2*(1-0.5)^(5-2)\n\n[1] 0.3125\n\n\n\n\n\n2.2.2 Cumulative Distribution Function (pbinom)\nPROBLEM 14: If you flip a coin \\(n=5\\) times and in each flip the probability of heads is \\(p=0.5\\), what is the chance that you get at most 2 successes?\n\n\nSolution:\n\nNow we want to find \\(P(X\\leq2)\\). We know that \\(P(X\\leq2)=P(X=2)+P(X=1)+P(X=0)\\), so we could again use the dbinom( ) function.\n\ndbinom(2, size=5, prob=0.5) + dbinom(1, size=5, prob=0.5) + dbinom(0, size=5, prob=0.5)\n\n[1] 0.5\n\n\nThe problem is that this approach becomes cumbersome as the number of trials increases. A more efficient approach is to recognize that \\(P(X\\leq2)\\) takes the form of the CDF and use pnorm( ).\n\npbinom(2, size=5, prob=0.5)\n\n[1] 0.5\n\n\n\nPROBLEM 15: If you flip a coin \\(n=100\\) times and in each flip the probability of heads is \\(p=0.25\\), what is the chance that you get at most 20 successes?\n\n\nSolution:\n\n\npbinom(20, size=100, prob=0.25)\n\n[1] 0.1488311\n\n\n\nPROBLEM 16: If you flip a coin \\(n=100\\) times and in each flip the probability of heads is \\(p=0.25\\), what is the chance that you get at least 20 successes?\n\n\nSolution:\n\nWe have two ways to solve this problem. First, we can write \\(P(X\\geq 20)=1-P(X&lt;20)=1-P(X\\leq 19)\\) where \\(P(X&lt;20)=P(X\\leq 19)\\) since \\(X\\) is discrete.\n\n1-pbinom(19, size=100, prob=0.25)\n\n[1] 0.9004696\n\n\nAlternatively, we can use the lower.tail=FALSE option to tell R we want the probability greater than x. However, note that this is strictly greater than, so we must again remember than \\(P(X\\geq 20)=P(X&gt;19)\\).\n\npbinom(19, size=100, prob=0.25, lower.tail=FALSE)\n\n[1] 0.9004696\n\n\n\n\n\n2.2.3 Quantile Function (qbinom)\nPROBLEM 17: Suppose you flip a coin \\(n=20\\) times where each flip has a probability of heads equal to \\(p=0.5\\). Find the value \\(q\\) such that the probability of getting at most \\(q\\) successes is equal to 0.25.\n\n\nSolution:\n\n\nqbinom(0.25, size=20, prob=0.5)\n\n[1] 8\n\n\n\n\n\n2.2.4 Random Number Generator (rbinom)\nPROBLEM 18: Generate \\(n=50\\) Bernoulli\\((p)\\) random variables with \\(p=0.2\\).\n\n\nSolution:\n\n\nx = rbinom(50, size=1, prob=0.2)\nx\n\n [1] 0 0 0 0 1 0 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n[39] 0 0 0 1 0 1 1 0 1 0 0 0\n\nbarplot(table(x))\n\n\n\n\n\n\n\n\n\nPROBLEM 19: Generate \\(n=100\\) Binomial\\((n,p)\\) random variables with \\(p=0.4\\).\n\n\nSolution:\n\n\nx = rbinom(100, size=100, prob=0.2)\nx\n\n  [1] 25 18 18 19 15 25 19 24 25 21 27 17 19 18 18 14 25 17 18 20 20 13 16 13 20\n [26] 17 19 23 13 21 27 17 22 15 23 16 23 20 21 26 20 17 20 23 21 25 18 16 16 27\n [51] 24 23 17 20 20 22 20 23 16 17 20 21 19 14 24 16 19 18 18 20 24 18 19 16 19\n [76] 23 17 19 14 20 27 24 20 16 15 17 13 19 16 20 14 25 21 15 14 24 18 19 16 15\n\nbarplot(table(x))",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Computing Probabilities</span>"
    ]
  },
  {
    "objectID": "03probability.html",
    "href": "03probability.html",
    "title": "3  Sampling Distributions and the CLT",
    "section": "",
    "text": "3.1 Sampling Distributions\nSampling distributions are theoretical objects that represent the probability distribution of a statistic (usually the sample mean).\nThe sampling distribution of the sample mean is the theoretical distribution of means that would result from taking all possible samples of size \\(n\\) from the population.\nWe can build some intuition for what this means in R by simulating this sampling distribution.\nLet’s start by assuming that we are sampling from the normal distribution with mean \\(\\mu=0\\) and variance \\(\\sigma^2=1\\) such that observations in our sample are IID. A specific sample of size \\(n\\) then is going to consist of realizations \\(\\{x_1,\\dots,x_n\\}\\), where each \\(x_i\\) is a realization of the random variable \\(X_i \\sim N(0,1)\\).\nIn R, we now can take a sample of size \\(n=100\\) by simulating draws from the normal distribution. For this sample, we then also compute the mean defined by \\(\\frac{1}{n}\\sum_{i=1}^n x_i\\) and plot the distribution of the realizations \\(\\{x_1,\\dots,x_n\\}\\) in a histogram.\nx = rnorm(n=100, mean=0, sd=1)\nmean(x)\n\n[1] 0.1002256\n\nggplot(data.frame(x),aes(x)) + geom_histogram() \n\n\n\n\n\n\n\n# note: to plot the vector with ggplot, we convert x to a data.frame first\nWhile we cannot generate all possible samples of size \\(n\\) from this normal distribution to get the sampling distribution, we can many sample and get close to it.\nLet \\(R=100,000\\) be the number of samples we want to generate. The code below now constructs a for loop in R to do the following: (1) generate a random sample of size \\(n\\) from the population; (2) compute the sample mean and store the computed mean in a vector called xbar.\nR = 100000\nxbar = double(R)\nfor(r in 1:R){\n    x = rnorm(n=100, mean=0, sd=1) # generate new sample \n    xbar[r] = mean(x)              # calculate mean and ad it in vector xbar  \n}\nggplot(data.frame(xbar),aes(xbar),xtitle=\"mean in sample\") + \n  geom_histogram(title=\"Sampling distribution\") +\n  ggtitle(\"Sampling distribution\") +\n  xlab(\"Sample means\")\nThis plot is the distribution of sample means after taking \\(R=100,000\\) samples with size \\(n=100\\) from the population.\nNotice that this again looks like a normal distribution. The mean looks the same as in the first histogram within a single sample, but the variance looks much smaller.\nIn fact, when the population distribution is \\(N(\\mu,\\sigma^2)\\), then the distribution of \\(\\bar{X}=\\frac{1}{n}\\sum_{i=1}^nX_i\\) is \\(N(\\mu,\\sigma^2/n)\\). In this case, we know that \\(E(\\bar{X})=E(X)=\\mu=0\\) and \\(Var(\\bar{X})=Var(X)/n=\\sigma^2/n=1/100=0.01\\).\nLet’s check to see what the mean and variance are in our approximation of the sampling distribution.\nmean(xbar)\n\n[1] 7.508259e-05\n\nvar(xbar)\n\n[1] 0.009974603\nVery close to the true values!",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Sampling Distributions and the CLT</span>"
    ]
  },
  {
    "objectID": "03probability.html#central-limit-theorem-clt",
    "href": "03probability.html#central-limit-theorem-clt",
    "title": "3  Sampling Distributions and the CLT",
    "section": "3.2 Central Limit Theorem (CLT)",
    "text": "3.2 Central Limit Theorem (CLT)\nThe next question to consider is what if our population distribution was not normally distributed? What if it was skewed to the right or left?\nLet’s assume \\(X_i\\) has a Exponential\\((\\beta)\\) distribution where \\(\\beta=1\\) is a rate parameter. We can again take one individual sample \\({x_1,\\dots,x_n}\\).\n\nx = rexp(n=100, rate=1) # take draws\nggplot(data.frame(x),aes(x)) + geom_histogram() \n\n\n\n\n\n\n\n\nBased on this sample, it appears that distribution is highly asymmetric and skewed to the right.\nIn this case, should we still expect the sampling distribution of the sample mean to be normal? Let’s go through the same exercise as before, i.e. take many samples of size \\(n=100\\), compute the sample mean for each of the samples, and then plot the histogram of the sample means.\n\nR = 100000\nxbar = double(R)\nfor(r in 1:R){\n  x = rexp(n=100, rate=1)\n  xbar[r] = mean(x)\n}\nggplot(data.frame(xbar),aes(xbar),xtitle=\"mean in sample\") + \n  geom_histogram(title=\"Sampling distribution\") +\n  ggtitle(\"Sampling distribution\") +\n  xlab(\"Sample means\")\n\n\n\n\n\n\n\n\nThe distribution of sample means again looks normal!\nThe Central Limit Theorem guarantees that this will be the case as the sample size \\(n\\) gets large. Then for any population distribution, we know that the distribution of \\(\\bar{X}\\) will be approximately normal with mean \\(E(\\bar{X})=E(X)\\) and \\(Var(\\bar{X})=Var(X)/n\\).\nSince \\(X\\sim\\text{Exponential}(\\beta)\\) with \\(\\beta=1\\), it can be shown that \\(E(X)=\\beta=1\\) and \\(Var(X)=\\beta^2=1\\). Therefore, \\(\\bar{X}\\) is approximately normal with mean \\(E(\\bar{X})=E(X)=1\\) and variance \\(Var(\\bar{X})=Var(X)/n=1/100=0.01\\).\nWe again check these results using our approximate sampling distribution and find consistent answers.\n\nmean(xbar)\n\n[1] 1.000052\n\nvar(xbar)\n\n[1] 0.0100667",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Sampling Distributions and the CLT</span>"
    ]
  },
  {
    "objectID": "04estimation.html",
    "href": "04estimation.html",
    "title": "4  Estimation",
    "section": "",
    "text": "4.1 Point Estimation\nPoint estimation is all about using one single number (statistic) to estimate a population parameter of interest. Generally, we need to differentiate three different terms:\n\\(\\theta\\) is the population parameter we want to know. \\(\\hat{\\theta}(X)\\) is our estimator, which is a function of our random sample. It is itself a random variable because another random sample will lead to a different number. The estimate \\(\\hat{\\theta}(x)\\) then is our statistic that we compute for given sample; given realizations \\((x_1,\\dots,x_n)\\), this number is not random. Note that we apply the same function \\(g()\\) in the estimator and estimate, but in the estimator the \\(X_i\\) are random variables whereas in the estimate the \\(x_i\\) are realizations.\nTo highlight the difference between estimate and estimator consider the following example: the population is 1,000,000 consumers and we want to know how many of them purchased our product. Hence, the population parameter we want to know is \\[\\theta = \\frac{\\text{no. purchases in population}}{1,000,000}\\]\nIn principle, we could just call all 1,000,000 consumers and ask, but that would cost way too much time (and time is money)! Instead, we are going to take a random sample with \\(n\\) consumers from the population. In this case, our estimator for the mean is \\[\\hat{\\theta}(X) = g(X_1,\\dots,X_n) = \\frac{\\text{no. purchases in sample}}{n} \\]\nThis is what we now do in R.\n# Generate population and calculate population parameter\npop = rbinom(1000000,1,0.2) # generates 1,000,000 consumers that bought (=1) or not(=0)\npop_par = mean(pop)  \npop_par \n\n[1] 0.200342\n\n# Take random sample and calculate estimate on that sample\nx = sample(pop,100,replace=TRUE)    # randomly takes 100 consumers out of pop \nestimate = mean(x)\nestimate \n\n[1] 0.17\n\n# Calculate sampling error \nsampling_error = abs(estimate-pop_par) \nsampling_error\n\n[1] 0.030342\nBoth the estimate and sampling error will be different if we take another random sample.\nx = sample(pop,100,replace=TRUE)\nmean(x)\n\n[1] 0.28",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Estimation</span>"
    ]
  },
  {
    "objectID": "04estimation.html#point-estimation",
    "href": "04estimation.html#point-estimation",
    "title": "4  Estimation",
    "section": "",
    "text": "Parameter: \\(\\theta\\)\nEstimator: \\(\\hat{\\theta}(X) = g(X_1,\\dots,X_n)\\)\nEstimate: \\(\\hat{\\theta}(x) = g(x_1,\\dots,x_n)\\)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Estimation</span>"
    ]
  },
  {
    "objectID": "04estimation.html#confidence-intervals",
    "href": "04estimation.html#confidence-intervals",
    "title": "4  Estimation",
    "section": "4.2 Confidence Intervals",
    "text": "4.2 Confidence Intervals\nAs we just saw, estimators themselves are random variables and subject to variation across different samples. Hence, point estimates are not enough to learn about the population parameter we are interested in. Rather than provide a single point estimate, it will be better to provide a range of plausible values for the parameter of interest. This is the idea of confidence intervals.\nA confidence interval for any parameter \\(\\theta\\) will always take the form: \\[\\hat{\\theta}\\pm \\text{(critical value)}\\times \\text{SD}(\\hat{\\theta})\\] where \\(\\hat{\\theta}\\) is an estimator of \\(\\theta\\), the critical value is a quantile of a normal or t distribution, and \\(\\text{SD}(\\hat{\\theta})\\) is the standard deviation of the estimator.\n\n4.2.1 Types of Confidence Intervals\nWe will consider four types of \\((1-\\alpha)100\\%\\) confidence intervals.\n\nConfidence interval for mean \\(\\mu\\), data are normally distributed, variance \\(\\sigma^2\\) is known \\[\\bar{x}\\pm z_{\\alpha/2}{\\sigma\\over\\sqrt{n}}\\]\nConfidence interval for mean \\(\\mu\\), data are normally distributed, variance \\(\\sigma^2\\) is unknown \\[\\bar{x}\\pm t_{n-1,\\alpha/2}{s\\over\\sqrt{n}}\\]\nConfidence interval for mean \\(\\mu\\), data are not normally distributed \\[\\bar{x}\\pm t_{n-1,\\alpha/2}{s\\over\\sqrt{n}}\\]\nConfidence interval for proportion \\(p\\), data are binary (0s and 1s) \\[\\hat{p}\\pm z_{\\alpha/2}\\sqrt{\\hat{p}(1-\\hat{p})\\over n}\\]\n\nNotice that we can compute the critical values in R. If we need \\(z_{\\alpha/2}\\), then we must find the value of a standard normal random variable such that \\(\\alpha/2\\) percent of the area is to its right. This is can be found using the normal quantile function qnorm( )!\nFor example, if \\(\\alpha=0.05\\) let’s use qnorm( ) to find \\(z_{0.025}\\).\n\nqnorm(0.025, mean=0, sd=1, lower.tail=FALSE)\n\n[1] 1.959964\n\n\nA summary of the most commonly used normal critical values are provided below.\n\n\n\nConfidence Level $(1-\\alpha)$\nCritical Value $z_{\\alpha/2}$\n\n\n\n\n99%\n2.576\n\n\n95%\n1.96\n\n\n90%\n1.645\n\n\n\n\n\nSimilarly, if we need to compute \\(t_{n-1,\\alpha/2}\\) with \\(\\alpha=0.05\\) and our data have \\(n=50\\) observations, we can use the qt( ) function.\n\nqt(0.025, df=50-1, lower.tail=FALSE)\n\n[1] 2.009575\n\n\nNotice this critical value is larger than \\(z_{0.025}\\) – this comes from the fact that the t-distribution has fatter tails than the normal distribution. But, it also turns out that a t distribution (which only has one parameter \\(\\nu\\) called the “degrees of freedom”) converges to a normal distribution as \\(\\nu\\to\\infty\\). We can formally check this using qt( ).\n\nqt(0.025, df=50, lower.tail=FALSE)\n\n[1] 2.008559\n\nqt(0.025, df=100, lower.tail=FALSE)\n\n[1] 1.983972\n\nqt(0.025, df=1000, lower.tail=FALSE)\n\n[1] 1.962339\n\nqt(0.025, df=10000, lower.tail=FALSE)\n\n[1] 1.960201\n\n\nNotice how these values approach \\(z_{0.025}\\approx1.96\\) as the degrees of freedom parameter gets large.\n\n\n4.2.2 Interpreting confidence intervals\nSuppose an analyst tells you that the 95% confidence interval for the population parameter \\(\\theta\\) is \\((0.5,1.5)\\). What can you conclude from this, i.e., what does it mean that we are 95% confident that \\(\\theta\\) is in this interval?\nWhat the confidence interval tells us is that if we take many samples from the population and construct the confidence interval for each sample, then in approximately 95% of the samples the constructed confidence interval will include the true population parameter. Note that, same as the estimator, the confidence interval depends on the sample and hence is random.\nWe can check in R whether this holds using the same population we generated earlier. For this, we are now going to construct a for loop that does the following: (1) take a random sample of size \\(n=100\\) from the population, (2) compute the 95% confidence interval (type 4 above) for the sample, check whether the population parameter lies within the computed confidence interval and store the result in a vector called poppar_in_ci.\n\nR = 1000                                # how many samples to take \npoppar_in_ci = double(R)\nfor(r in 1:R){\n    x = sample(pop,100,replace=TRUE)                   # generate new sample \n    phat = mean(x)                        \n    s = sqrt(phat*(1-phat)/100)           # see formula for CI of proportion above\n    poppar_in_ci[r] = phat -1.96 * s &lt; pop_par & \n                        phat + 1.96 * s &gt; pop_par\n}\nmean(poppar_in_ci)\n\n[1] 0.933\n\n\nThe result of this simulation shows that for approximately 95% of our samples, the constructed confidence interval indeed contains the population parameter value.\nNote that in this case, we were sampling from a population we first generated. Often, we take samples just from a hypothetical population, where we assume that the samples are drawn from a distribution that captures the population. For example, in chapter 3 we directly made assumptions on the population distribution and sampled from the respective distributions.\n\n\n4.2.3 Examples\nPROBLEM 1: A wine importer needs to report the average percentage of alcohol in bottles of French wine. From experience with previous kinds of wine, the importer believes that alcolhol percentages are normally distributed and the population standard deviation is 1.2%. The importer randomly samples 60 bottles of the new wine and obtains a sample mean \\(\\bar{x}=9.3\\%\\). Give a 90% confidence interval for the average percentage of alcohol in all bottles of the new wine.\n\n\nSolution:\n\nFrom the problem, we know the following. \\[\\begin{align*}\nn = 60\\\\\n\\sigma=1.2\\\\\n\\bar{x} =9.3\\\\\n\\alpha=0.10\n\\end{align*}\\] We must first figure out which type of confidence interval to use. Notice that we are trying to estimate the average percentage of alcohol, so our parameter is a mean \\(\\mu\\). Moreover, we are told to assume that the data are normally distributed and the population standard deviation \\(\\sigma\\) is known. Thefore, our confidence interval will be of the form: \\[\\bar{x}\\pm z_{\\alpha/2}{\\sigma\\over\\sqrt{n}}.\\] We can now define each object in R and construct the confidence interval.\n\nn = 60\nsigma = 1.2\nxbar = 9.3\nzalpha = qnorm(0.05, mean=0, sd=1, lower.tail=FALSE)\nxbar - zalpha*sigma/sqrt(n)\n\n[1] 9.04518\n\nxbar + zalpha*sigma/sqrt(n)\n\n[1] 9.55482\n\n\nTherefore, we are 90% confident that the true average alcohol content in all new bottles of wine is between 9.05% and 9.55%.\n\nPROBLEM 2: An economist wants to estimate the average amount in checking accounts at banks in a given region. A random sample of 100 accounts gives \\(\\bar{x}=£357.60\\) and \\(s=£140.00\\). Give a 95% confidence interval for \\(\\mu\\), the average amount in any checking account at a bank in the given region.\n\n\nSolution:\n\nFrom the problem, we know the following. \\[\\begin{align*}\nn &= 100\\\\\n\\bar{x} &=357.60\\\\\ns &= 140\\\\\n\\alpha&=0.5\n\\end{align*}\\] Here we are not told whether the data are normally distributed. However, it won’t matter because we only have an estimate of \\(\\sigma\\) (remember that among the four types of confidence intervals we considered earlier, there are no differences between case II and III). Therefore, our confidence interval will be of the form: \\[\\bar{x}\\pm t_{n-1,\\alpha/2}{s\\over\\sqrt{n}}.\\] We can again define each object in R and construct the confidence interval.\n\nn = 100\nxbar = 357.60\ns = 140\ntalpha = qt(0.025, df=n-1, lower.tail=FALSE)\nxbar - talpha*s/sqrt(n)\n\n[1] 329.821\n\nxbar + talpha*s/sqrt(n)\n\n[1] 385.379\n\n\nTherefore, we are 95% confident that the true average account checking account value in the given region is between £329.82 and £385.38.\n\nPROBLEM 3: The EuStockMarkets data set in R provides daily closing prices of four major European stock indices: Germany DAX (Ibis), Switzerland SMI, France CAC, and UK FTSE. Using this data set, produce a 99% confidence interval for the average closing price of the UK FTSE.\n\n\nSolution:\n\nFirst, let’s load in the data from R.\n\ndata(EuStockMarkets)\nhead(EuStockMarkets)\n\n\n\n\n\n\nDAX\nSMI\nCAC\nFTSE\n\n\n\n\n1628.75\n1678.1\n1772.8\n2443.6\n\n\n1613.63\n1688.5\n1750.5\n2460.2\n\n\n1606.51\n1678.6\n1718.0\n2448.2\n\n\n1621.04\n1684.1\n1708.1\n2470.4\n\n\n1618.16\n1686.6\n1723.1\n2484.7\n\n\n1610.61\n1671.6\n1714.3\n2466.8\n\n\n\n\n\n\n\nNow let’s pull the subset of data we care about (i.e., the UK FTSE column).\n\nuk = EuStockMarkets[,4]\n\nNotice that we are not told anything about the true distribution of the data. Therefore, our confidence interval will be of the form: \\[\\bar{x}\\pm t_{n-1,\\alpha/2}{s\\over\\sqrt{n}}.\\] Next, let’s compute each component necessary to construct the 99% confidence interval.\n\nn = length(uk)\nxbar = mean(uk)\ns = sd(uk)\ntalpha = qt(0.005, df=n-1, lower.tail=FALSE)\nxbar - talpha*s/sqrt(n)\n\n[1] 3507.248\n\nxbar + talpha*s/sqrt(n)\n\n[1] 3624.038\n\n\nTherefore, we are 99% confident that the true closing price for the UK FTSE index is between £3,507.25 and £3,624.04.\nFinally, notice that we can use the t.test( ) function to perform the same analysis but with fewer ste",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Estimation</span>"
    ]
  },
  {
    "objectID": "05testing.html",
    "href": "05testing.html",
    "title": "5  Hypothesis Tests",
    "section": "",
    "text": "5.1 Steps of Hypothesis Testing\nStatistical hypothesis testing provides a rigorous framework for using data to provide evidence for or against claims.\nFor example, suppose that you are working for a start-up that develops education software for children. You’re working on a new software package and are now trying to determine how much to charge. Based on experience and market trends, the leadership team thinks £50 is reasonable. As the data scientist, you are asked to do some research.\nThe plan is for you to conduct a survey to check how much people would be willing to pay for the software. The leadership team will plan to charge £50 unless there is substantial evidence that people are willing to pay more. Your objective is to use the survey data to determine if the company should re-think the £50 price point.\nYou design a survey and send it to \\(n=30\\) potential customers. After everyone has responded, you find that the average willingess to pay in your sample is \\(\\bar{x}=55.7\\) pounds and \\(s^2=64.8\\).\nn = 30\nxbar = 55.7\ns2 = 64.8\nNow, what does this mean? We know that we cannot stop here and conclude that people are willing to pay more than £50 because if we had asked a different group of customers, our sample mean could change (and perhaps be lower than £50).\nOur approach will be to carry out a hypothesis test to formally decide what to do.\nStep 1: State the null and alternative hypotheses\nThe framework of hypothesis testing requires us to specify two mutually exclusive hypotheses: the null hypothesis \\(H_0\\) and the alternative hypothesis \\(H_1\\). Specifically, we should choose \\(H_0\\) to be the case of “no effect” or “no change” and choose \\(H_1\\) to be the case of what we want to show.\nHere, we are investigating whether people are willing to pay more than £50 on average so \\(\\mu&gt;50\\) will constitute the alternative.\n\\[\\begin{align*}\nH_0:&\\ \\mu\\leq50\\\\\nH_1:&\\ \\mu&gt;50\n\\end{align*}\\]\nStep 2: Choose a test and significance level\nTo determine which test is appropriate, we must first address the following questions.\nIn our case, we only have one parameter \\(\\mu\\) (the average WTP in the population) and we do not know the population variance, but have an estimate of it in our sample. Therefore, we should use a one-sample \\(t\\)-test.\n\\[t_{df} = {\\bar{X}-\\mu\\over S/\\sqrt{n}}\\]\nFinally, we will choose \\(\\alpha=0.01\\) so that we are fairly confident that if we detect deviations from £50, they reflect real deviations in the population.\nStep 3: Compute the observed test statistic\nSince we are using a one-sample \\(t\\)-test, our observed test statistic is: \\[t_{obs}={\\bar{x}-\\mu_0\\over s/\\sqrt{n}} = {55.7 - 50\\over \\sqrt{64.8}/\\sqrt{30}}=3.878\\]\nt_obs = (xbar - 50)/(sqrt(s2/n))\nt_obs\n\n[1] 3.878359\nOur observed test statistic provides a measure of “evidence” against the null hypothesis. In particular, we know that under the null hypothesis, the test statistic follows a \\(t_{df}=t_{n-1}=t_{29}\\) distribution. This distribution (plotted below) represents the distribution of sample evidence given that the null is true. Our observed test statistic (the dashed red line) shows that the event we observed (\\(\\bar{x}=55.7, s^2=64.8\\)), seems fairly unlikely under the null.\nOur next task will be to compute this probability more formally.\nStep 4: Calculate the p-value\nThe p-value is the probability of getting sample evidence as or more extreme than what we actually observed given that the null hypothesis is actually true. Remember that the test statistic is our measure of “sample evidence” – as the observed test statistic gets large, that will provide more evidence against the null hypothesis.\nSince we are working with a “greater-than” alternative, our p-value will be\n\\[\\begin{align*}\n\\text{p-value}\n&= P(t_{df}&gt;t_{obs}\\ | \\ H_0 \\text{ is true})\\\\\n&= P\\left(t_{n-1}&gt;{\\bar{x}-\\mu_0\\over s/\\sqrt{n}}\\ \\big|\\ \\mu\\leq50\\right)\\\\\n&= P\\left(t_{29}&gt;{55.7 - 50\\over \\sqrt{64.8}/\\sqrt{30}}\\right)\\\\\n&= P\\left(t_{29}&gt;3.878\\right)\\\\\n&\\approx 0.0003\n\\end{align*}\\]\npt(t_obs,df=n-1,lower.tail=FALSE)\n\n[1] 0.0002780401\nNotice that this value just corresponds to the region to the right of the observed test statistic. Since this probability is so small, it is hard to see the shaded area on the original plot. We can therefore create a “zoomed in” plot next to the original.\nStep 4: Make a statistical decision and interpret the results\nOnce the p-value has been calculated, the “decision rule” can be described as follows.\n\\[\\begin{align*}\n\\text{ if p-value }\\leq \\alpha &\\ \\text{ reject } H_0\\\\\n\\text{ if p-value }&gt; \\alpha &\\ \\text{ fail to reject } H_0\\\\\n\\end{align*}\\]\nWhere does this rule come from? Since \\(\\alpha\\) is the maximum p-value at which we reject \\(H_0\\), then we are ensuring that there is at most a \\(100\\alpha\\%\\) chance of committing a type I error. That is, if we found the p-value to be large, say 40%, then there would be a 40% chance of mistakenly deciding that the true WTP exceeded £50 when it in fact did not. For most problems, an error rate of 40% is too large to tolerate. In the social sciences, we normally choose \\(\\alpha \\in\\{0.1, 0.05, 0.01\\}\\) which corresponds to error rates of 10%, 5%, and 1%.\nIn the context of this problem, we find the p-value is roughly 0.03%. This means that if the true average WTP in the population is less than £50, there is a 0.03% chance that we would have observed sample evidence as or more extreme than what we did observe (\\(\\bar{x}=55.7, s^2=64.8\\)). This is very small – in fact, much smaller than the 5% error rate we can tolerate. Therefore, we decide to reject the null hypothesis and conclude that it is more likely that the true average WTP in the population exceeds £50.\nWe can take these results back to the leadership team in our company to convince them that they should consider raising the price.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Hypothesis Tests</span>"
    ]
  },
  {
    "objectID": "05testing.html#steps-of-hypothesis-testing",
    "href": "05testing.html#steps-of-hypothesis-testing",
    "title": "5  Hypothesis Tests",
    "section": "",
    "text": "How many parameters do we have? (one = one-sample test, two = two-sample test)\nDo we know the population variance? (yes = \\(z\\)-test, no = \\(t\\)-test)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Hypothesis Tests</span>"
    ]
  },
  {
    "objectID": "05testing.html#connection-to-confidence-intervals",
    "href": "05testing.html#connection-to-confidence-intervals",
    "title": "5  Hypothesis Tests",
    "section": "5.2 Connection to Confidence Intervals",
    "text": "5.2 Connection to Confidence Intervals\nThere is an intimate connection between hypothesis tests and confidence intervals. We will now go through the details to see why.\nTo start, remember that our decision rules for hypothesis testing take the following form.\n\\[\\begin{align*}\n\\text{ if p-value }\\leq \\alpha &;\\ \\text{ reject } H_0\\\\\n\\text{ if p-value } &gt; \\alpha &;\\ \\text{ fail to reject } H_0\\\\\n\\end{align*}\\]\nThis can also be described visually. Suppose you carry out a two-sided hypothesis test with \\(\\alpha=0.05\\) and compute a test statistic \\(z_{obs}=2.054\\) and a corresponding p-value equal to 0.04. This corresponds to a total area equal to 0.04 in the lower and upper tails of the distribution of the test statistic.\n\n\n\n\n\n\n\n\n\nWe can also work out what value (on the x-axis) corresponds to an area of \\(\\alpha/2=0.05/2=0.025\\) in the upper tail.\n\nqnorm(0.025,lower.tail=FALSE)\n\n[1] 1.959964\n\n\n\n\n\n\n\n\n\n\n\nNow the dotted black line is at the point \\(z_{\\alpha/2}=1.96\\) – i.e., the value such that the upper tail area is equal to \\(\\alpha/2=0.025\\). Notice that our shaded area falls to the right of this line, so by our decision rule, we would reject the null.\nBut, notice that we would reject the null for any test statistic (solid red line) that falls to the right of the critical value \\(z_{\\alpha/2}\\) (dotted black line).\nTherefore, the following would be an equivalent set of decision rules.\n\\[\\begin{align*}\n\\text{ if } \\big|z_{obs}\\big|\\geq z_{\\alpha/2} &\\ \\text{ reject } H_0\\\\\n\\text{ if } \\big|z_{obs}\\big| &lt; z_{\\alpha/2} &\\ \\text{ fail to reject } H_0\\\\\n\\end{align*}\\]\nRemember that a confidence interval is a range of plausible values, which we can now formally define as the the range of parameter values that would not be rejected by our hypothesis test. In this case, the “acceptance region” is defined as follows.\n\\[\\begin{align*}\n\\big|z_{obs}\\big|&lt; z_{\\alpha/2}\n&\\implies \\left|{\\bar{x} - \\mu_0 \\over \\sigma/\\sqrt{n}}\\right|&lt;z_{\\alpha/2}\\\\\n&\\implies -z_{\\alpha/2}&lt;{\\bar{x} - \\mu_0 \\over \\sigma/\\sqrt{n}}&lt;z_{\\alpha/2}\\\\\n&\\implies -z_{\\alpha/2} \\times {\\sigma \\over \\sqrt{n}} &lt;\\bar{x} - \\mu_0 &lt; z_{\\alpha/2} \\times {\\sigma \\over \\sqrt{n}}\\\\\n&\\implies -\\bar{x}-z_{\\alpha/2} \\times {\\sigma \\over \\sqrt{n}} &lt; - \\mu_0 &lt; -\\bar{x} + z_{\\alpha/2} \\times {\\sigma \\over \\sqrt{n}}\\\\\n&\\implies \\bar{x}-z_{\\alpha/2} \\times {\\sigma \\over \\sqrt{n}} &lt; \\mu_0 &lt; \\bar{x} + z_{\\alpha/2} \\times {\\sigma \\over \\sqrt{n}}\n\\end{align*}\\] This last line is the exact form of a confidence interval!",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Hypothesis Tests</span>"
    ]
  },
  {
    "objectID": "05testing.html#hypothesis-testing-in-r",
    "href": "05testing.html#hypothesis-testing-in-r",
    "title": "5  Hypothesis Tests",
    "section": "5.3 Hypothesis Testing in R",
    "text": "5.3 Hypothesis Testing in R\nWe can use the t.test( ) function to carry out both one and two-sample \\(t\\)-tests in R. (Note: There are no built-in \\(z\\)-test functions in R because when we work with real data, we never know the population variance!)\n\nONE-SAMPLE \\(t\\)-TEST t.test(mydata, alternative, mu, conf.level) mydata: data on the variable of interest alternative: what type of alternative hypothesis is specified? (options: “two.sided”, “greater”, “less”) mu: the value of \\(\\mu\\) under the null hypothesis conf.level: confidence level of the test (\\(1-\\alpha\\))\nTWO-SAMPLE \\(t\\)-TEST t.test(mydata1, mydata2, alternative, mu, conf.level) mydata1: data on the first variable of interest mydata2: data on the second variable of interest alternative: what type of alternative hypothesis is specified? (options: “two.sided”, “greater”, “less”) mu: the value of the difference \\(\\mu_1-\\mu_2\\) under the null hypothesis conf.level: confidence level of the test (\\(1-\\alpha\\))\n\nPROBLEM 1: The EuStockMarkets data set in R provides daily closing prices of four major European stock indices: Germany DAX (Ibis), Switzerland SMI, France CAC, and UK FTSE. Using this data set, test to see if there are differences in the closing prices of the SMI and CAC indices. Carry out this test at the 5% significance level and do not assume equal variances.\n\n\nSolution:\n\n\n# load the data\ndata(EuStockMarkets)\n\n# create the SMI variable which is the second column of the EuStockMarkets data\nSMI = EuStockMarkets[,2]\n\n# create the CAC variable which is the third column of the EuStockMarkets data\nCAC = EuStockMarkets[,3]\n\n# execute the two-sample t-test\nt.test(SMI, CAC, alternative=\"two.sided\", mu=0, conf.level=0.95)\n\n\n    Welch Two Sample t-test\n\ndata:  SMI and CAC\nt = 28.119, df = 2305.1, p-value &lt; 2.2e-16\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n 1068.307 1228.484\nsample estimates:\nmean of x mean of y \n 3376.224  2227.828 \n\n\nWe find a p-value much smaller than \\(\\alpha=0.05\\), so we can reject the null and conclude that there are differences in the closing prices between the Swiss SMI and French CAC stock indices.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Hypothesis Tests</span>"
    ]
  },
  {
    "objectID": "06regression.html",
    "href": "06regression.html",
    "title": "6  Regression",
    "section": "",
    "text": "6.1 Linear Regression\nRegression models are useful tools for (1) understanding the relationship between a response variable \\(Y\\) and a set of predictors \\(X_1,\\dotsc,X_p\\) and (2) predicting new responses \\(Y\\) from the predictors \\(X_1,\\dotsc,X_p\\).\nWe’ll start with linear regression, which assumes that the relationship between \\(Y\\) and \\(X_1,\\dotsc,X_p\\) is linear.\nLet’s consider a simple example where we generate data from the following regression model.\n\\[Y = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\varepsilon\\]\nTo generate data from this model, we first need to set the “true values” for the model parameters \\((\\beta_0, \\beta_1, \\beta_2)\\), generate the predictor variables \\((X_1, X_2)\\), and generate the error term \\((\\varepsilon)\\).\nOnce we have fixed the true values of the parameters and generated predictor variables and the error term, the regression formula above tells us how to generate the response variable \\(Y\\).\nn = 100\nbeta0 = -5\nbeta1 = 2\nbeta2 = -1\nX1 = runif(n,min=-1,max=1)\nX2 = runif(n,min=-1,max=1)\nepsilon = rnorm(n)\nY = beta0 + beta1*X1 + beta2*X2 + epsilon\nNow let’s inspect the data.\npairs(Y ~ X1 + X2)\nAs we should expect, we find a positive relationship between \\(Y\\) and \\(X_1\\), a negative relationship between \\(Y\\) and \\(X_2\\), and no relationship between \\(X_1\\) and \\(X_2\\) (since they are uncorrelated).\nNow let’s formally estimate the model parameters \\((\\beta_0,\\beta_1,\\beta_2)\\) using R’s built-in linear model function lm( ).\nlm.fit = lm(Y ~ X1 + X2)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = Y ~ X1 + X2)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.25655 -0.62299  0.03299  0.57096  2.08334 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -4.91360    0.09101 -53.990  &lt; 2e-16 ***\nX1           1.66560    0.15890  10.482  &lt; 2e-16 ***\nX2          -1.00377    0.15743  -6.376 6.16e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.8925 on 97 degrees of freedom\nMultiple R-squared:  0.601, Adjusted R-squared:  0.5927 \nF-statistic: 73.05 on 2 and 97 DF,  p-value: &lt; 2.2e-16\nParameter Estimates\nFirst, focus on the “Coefficients” section. Notice that in the first column R reports estimates of our model parameters: \\(\\hat{\\beta}_0=-4.914\\), \\(\\hat{\\beta}_1=1.666\\), and \\(\\hat{\\beta}_2=-1.004\\). Since we generated this data set, we know the “true” values are \\(\\beta_0=-5\\), \\(\\beta_1=2\\), and \\(\\beta_2=-1\\). The estimates here are pretty close to the truth. (Remember: the estimates will not exactly equal the true values because we only have a random sample of \\(n=100\\) observations!)\nInterpretation\nHow should we interpret the estimates? Since \\(\\hat{\\beta}_1=1.666\\), we would say that a one unit increase in \\(X_1\\) will lead to a 1.666 unit increase in \\(Y\\). Similarly, a one unit increase in \\(X_2\\) will lead to a 1.004 unit decrease in \\(Y\\). The only way to interpret the intercept is as the value of \\(Y\\) when the \\(X\\)’s are all set to zero. In many instances, setting \\(X=0\\) makes no sense, so we usually focus our attention on the coefficients attached to the predictor variables.\nSignificance\nIn the second, third, and fourth columns, R reports the standard error of \\(\\hat{\\beta}\\) and the t-statistic and p-value corresponding to a (one-sample) test of \\(H_0:\\beta=0\\) against \\(H_1:\\beta\\ne0\\). The asterisks next to the p-values indicate the levels (e.g., \\(\\alpha=0.05\\), \\(\\alpha=0.001\\)) for which we would conclude that the parameter is significantly different from zero. This test is naturally of interest in a regression setting because if \\(\\beta_2=0\\), for example, then \\(X_2\\) has no effect on the response \\(Y\\).\nModel Fit\nNow look at the last section where it says “Multiple R-squared: 0.601”. This value is the \\(R^2\\) statistic, which measures the percent of the variation in \\(Y\\) that is explained by the predictors. In this case, we find that 60.1% of the variation in \\(Y\\) can be explained by \\(X_1\\) and \\(X_2\\). In general, it is difficult to define an absolute scale for what a “good” \\(R^2\\) value is. In some contexts, 60% may be very high while in others it may be low. It likely depends on how difficult the response variable is to model and predict.\nPrediction\nSuppose I observed some new values of \\(X_1\\) and \\(X_2\\), say \\(X_1=0\\) and \\(X_2=0.5\\). How can I use the model to predict the corresponding value of \\(Y\\)?\nI could simply do the calculation by hand: \\[\\hat{Y}=\\hat{\\beta}_0 + \\hat{\\beta}_1X_1 + \\hat{\\beta}_2X_2 =-4.914 + 1.666(0) - 1.004(0.5)=-5.416\\] where we use the “hat” notation to denote estimates or predicted values.\nWe can also use built-in prediction tools in R (where any differences would just be due to rounding error).\npredict(lm.fit, newdata=data.frame(X1=0,X2=0.5))\n\n        1 \n-5.415489\nThe first argument of the predict( ) function is the regression object we created using the lm( ) function. The second argument is the new set of covariates for which we want to predict a new response \\(Y\\). (Note: the names of variables in newdata must be the same names used in the original data.)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Regression</span>"
    ]
  },
  {
    "objectID": "06regression.html#linear-regression",
    "href": "06regression.html#linear-regression",
    "title": "6  Regression",
    "section": "",
    "text": "parameters: \\(\\beta_0=-5\\), \\(\\beta_1=2\\), \\(\\beta_2=-1\\)\npredictor variables: \\(X_1\\sim Unif(-1,1)\\), \\(X_2\\sim Unif(-1,1)\\)\nerror term: \\(\\varepsilon\\sim N(0,1)\\)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Regression</span>"
    ]
  },
  {
    "objectID": "06regression.html#regression-trees",
    "href": "06regression.html#regression-trees",
    "title": "6  Regression",
    "section": "6.2 Regression Trees",
    "text": "6.2 Regression Trees\nA natural question to ask now is what happens if the “true” model that generated our data was not linear? For example, our model could look something like this:\n\\[Y_i = \\beta_0 + {\\beta_1X_{1i} \\over\\beta_2 + X_{2i}} + \\varepsilon_i\\]\nHere we still have three model parameters (\\(\\beta_0,\\beta_1,\\beta_2\\)), but they enter the regression function in a nonlinear fashion.\nIf we generate data from this model and then estimate the linear regression model from section 1, what will happen?\n\n# generate data\nn = 100\nbeta0 = -5\nbeta1 = 2\nbeta2 = -1\nX1 = runif(n,min=-1,max=1)\nX2 = runif(n,min=-1,max=1)\nepsilon = rnorm(n)\nY = beta0 + beta1*X1/(beta2+X2) + epsilon\n\n# estimate linear regression model\nlm.fit = lm(Y ~ X1 + X2)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = Y ~ X1 + X2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-47.926  -8.548  -1.961   4.484 165.499 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   -3.815      2.153  -1.772 0.079519 .  \nX1           -14.362      3.596  -3.994 0.000126 ***\nX2             5.733      3.510   1.634 0.105595    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 21.22 on 97 degrees of freedom\nMultiple R-squared:  0.1697,    Adjusted R-squared:  0.1526 \nF-statistic: 9.914 on 2 and 97 DF,  p-value: 0.0001209\n\n\nThe answer is that we get incorrect estimates of model parameters! (Remember \\(\\beta=-5,\\beta_1=2,\\beta_2=-1\\).)\nA more flexible approach to regression modeling is provided by regression trees. The idea is to split up the covariate space into homogeneous regions (with respect to the response \\(Y\\)) and then fit simple linear models within each region.\nWe can use the rpart library in R to fit and plot regression trees. You’ll actually notice a similar syntax between lm( ) and rpart( ).\n\nlibrary(rpart)\n\n# estimate regression tree\ntree.fit = rpart(Y ~ X1 + X2)\n\n# plot the estimated tree\nplot(tree.fit, uniform=TRUE, margin=.05)\ntext(tree.fit)\n\n\n\n\n\n\n\n\n\n\nThe output from a regression tree model looks very different from the output of a linear regression model. This is mostly because we had real-valued parameters in the linear model, but have much more complicated parameters in the tree model.\nThe top node is called the root node and indicates the most important variable for predicting \\(Y\\). Each subsequent node is called an interior node until you get to the last node showing a numeric value which is called a terminal node.\nTree models should be interpreted as a sequence of decisions for the purposes of making a prediction. Each node will present a logical statement and if that statement is true, we move down and to the left whereas if that statement is false, we move down and to the right.\nFor example, if you wanted to predict \\(Y\\) when \\(X_1=0\\) and \\(X_2=0.5\\), the root note first asks “Is \\(X_1\\geq -0.9113\\)?” If yes, then left and if no then right. Here our answer is yes, so we go to the next node to the left and ask “Is \\(X_1\\geq0.264\\)?” Our answer is no so we go to the right and ask \\(X_2&lt;0.6835\\)?” Our answer is yes so we go to the left. Since this represents the terminal node, we’re left with our predition of \\(\\hat{Y}=-4.037\\). That is, if \\(X_1=1\\) and \\(X_2=9\\) then the model predicts \\(\\hat{Y}=-4.037\\).\nWe can also use the predict( ) function as we did with the linear regression model above.\n\npredict(tree.fit, newdata=data.frame(X1=0,X2=0.5))\n\n        1 \n-4.036809",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Regression</span>"
    ]
  },
  {
    "objectID": "06regression.html#model-selection",
    "href": "06regression.html#model-selection",
    "title": "6  Regression",
    "section": "6.3 Model Selection",
    "text": "6.3 Model Selection\nLet’s see how regression trees compare to linear regression models in terms of out-of-sample prediction. We’ll consider two cases:\n\n\nThe true model is a linear model\n\nThe true model is a nonlinear model\n\n CASE A: TRUE MODEL IS LINEAR \nFirst, we’ll generate a training and test data set from a linear regression model as in section 1. The training data set will be used for estimation and the test data will be used for prediction.\n\nn = 100\nbeta0 = -5\nbeta1 = 2\nbeta2 = -1\nX1 = runif(n,min=-1,max=1)\nX2 = runif(n,min=-1,max=1)\nepsilon = rnorm(n)\nY = beta0 + beta1*X1 + beta2*X2 + epsilon\ntrain = data.frame(Y=Y[1:70], X1=X1[1:70], X2=X2[1:70])\ntest = data.frame(Y=Y[71:100], X1=X1[71:100], X2=X2[71:100])\n\nNow let’s estimate both the linear regression and regression tree models on the training data.\n\n# estimate linear regression model\nlm.fit = lm(Y ~ X1 + X2, data=train)\n\n# estimate regression tree model\ntree.fit = rpart(Y ~ X1 + X2, data=train)\n\nTo compare out-of-sample model performance, we’ll compute the root mean squared error (RMSE). \\[\\text{RMSE}=\\sqrt{{1\\over n}\\sum_{i=1}^n (\\hat{y}_i-y_i)^2}\\]\n\n# linear regression model\nlm.predict = predict(lm.fit, newdata=test)\nlm.rmse = sqrt(mean((lm.predict-test$Y)^2))\nlm.rmse\n\n[1] 0.9157128\n\n\n\n# regression tree model\ntree.predict = predict(tree.fit, newdata=test)\ntree.rmse = sqrt(mean((tree.predict - test$Y)^2))\ntree.rmse\n\n[1] 1.148858\n\n\nIn this case the linear regression model has better predictive performance, which is not too surprising because we simulated the data from that model!\n CASE B: TRUE MODEL IS NONLINEAR \nWe will again generate a training and test data set, but now from the nonlinear regression model we used in section 2.\n\nn = 100\nbeta0 = -5\nbeta1 = 2\nbeta2 = -1\nX1 = runif(n,min=-1,max=1)\nX2 = runif(n,min=-1,max=1)\nepsilon = rnorm(n)\nY = beta0 + beta1*X1/(beta2+X2) + epsilon\ntrain = data.frame(Y=Y[1:70], X1=X1[1:70], X2=X2[1:70])\ntest = data.frame(Y=Y[71:100], X1=X1[71:100], X2=X2[71:100])\n\nLet’s again estimate both the linear regression and regression tree models on the training data and compute the predictive RMSE.\n\n# linear regression model\nlm.fit = lm(Y ~ X1 + X2, data=train)\nlm.predict = predict(lm.fit, newdata=test)\nlm.rmse = sqrt(mean((lm.predict - test$Y)^2))\nlm.rmse\n\n[1] 18.76132\n\n\n\n# regression tree model\ntree.fit = rpart(Y ~ X1 + X2, data=train)\ntree.predict = predict(tree.fit, newdata=test)\ntree.rmse = sqrt(mean((tree.predict - test$Y)^2))\ntree.rmse\n\n[1] 17.35195\n\n\nNow the regression tree model has better predictive performance (but notice that the linear model still does relatively well!) In general, regression trees suffer from a problem called overfitting: the trees learn too much from the training data that they don’t generalize well to test data. There are ways of correcting for this, and you will learn more about them in Data Analyics II!",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Regression</span>"
    ]
  },
  {
    "objectID": "07classification.html",
    "href": "07classification.html",
    "title": "7  Classification",
    "section": "",
    "text": "7.1 \\(k\\)-Nearest Neighbors\nClassification shares many similarities with regression: We have a response variable \\(Y\\) and a set of one or more predictors \\(X_1,\\dotsc,X_p\\). The difference is that for classification problems, the response \\(Y\\) is discrete, meaning \\(Y\\in\\{1,2,\\dotsc,C\\}\\) where \\(C\\) is the number of classes that \\(Y\\) can take on.\nWe will focus our attention on binary responses \\(Y\\in\\{0,1\\}\\), but all of the methods we discuss can be extended to the more general case outlined above.\nTo illustrate classification methods, we will use the Default data in the ISLR R library. The data set contains four variables: default is an indicator of whether the customer defaulted on their debt, student is an indicator of whether the customer is a student, balance is the average balance that the customer has remaining on their credit card after making their monthly payment, and income is the customer’s income.\nWe also need to split up the data into training and test samples in order to measure the predictive accuracy of different approaches to classification.\nThe \\(k\\)-NN algorithms are built on the following idea: given a new observation \\(X^*\\) for which we want to predict an associated response \\(Y^*\\), we can find values of \\(X\\) in our data that look similar to \\(X^*\\) and then classify \\(Y^*\\) based on the associated \\(Y\\)’s. We will use Euclidean distance is a measure of similarity (which is only defined for real-valued \\(X\\)’s).\nLet’s take a small portion (first 10 rows) of the Default data to work through a simple example. Notice that we will exclude the student variable since it is a categorical rather than numeric variable. We will use the 11th observation as our “test” data \\(X^*\\) that we want to make predictions for.\nX = Default[1:10,3:4]\nY = Default[1:10,1]\nnewX = Default[11,3:4]\nWe now need to compute the similarity (i.e., Euclidean distance) between \\(X^*=(X_1^*,X_2^*)\\) and \\(X_i=(X_{1i},X_{2i})\\) for each \\(i=1,\\dotsc,n\\).\n\\[dist(X^*,X_i)=||X^*-X_i||=\\sqrt{(X_1^*-X_{1i})^2+(X_2^*-X_{2i})^2}\\]\nTo do this in R, we can take use the apply( ) function. The first argument is the matrix of \\(X\\) variables that we want to cycle through to compare with \\(X^*\\).\nThe second argument of the apply( ) function tells R whether we want to perform an operation for each row (=1) of for each column (=2). The last row tells R what function we want to compute. Here, we need to evaluate \\(dist(X^*,X_i)\\) for each row.\ndistance = apply(X,1,function(x)sqrt(sum((x-newX)^2)))\ndistance\n\n        1         2         3         4         5         6         7         8 \n22502.381  9799.072  9954.126 13843.541 16611.013 14408.889  3144.449  4346.510 \n        9        10 \n15640.610  7404.195\nNotice that the function returns a set of 10 distances. If we wanted to use the 1st-nearest neighbor classifier to predict \\(Y^*\\), for example, then we would need to find the \\(Y\\) value of \\(X_i\\) for the observation \\(i\\) that has the smallest distance. We can find that value using the which.min( ) function.\nwhich.min(distance)\n\n7 \n7 \n\nY[which.min(distance)]\n\n[1] No\nLevels: No Yes\nTherefore, we would predict \\(Y^*=No\\) having observed \\(X^*\\).\nNow let’s go back to the full data set and test the performance of the \\(k\\)-NN classifier. The first thing we should do is standardize the \\(X\\)’s since the nearest neighbors algorithm depends on the scale of the covariates.\nstdtrainX = scale(train[,3:4])\nstdtestX = scale(test[,3:4])\n\nsummary(stdtrainX)\n\n    balance             income        \n Min.   :-1.72782   Min.   :-2.46318  \n 1st Qu.:-0.73329   1st Qu.:-0.92073  \n Median :-0.03045   Median : 0.08042  \n Mean   : 0.00000   Mean   : 0.00000  \n 3rd Qu.: 0.68581   3rd Qu.: 0.77299  \n Max.   : 3.45400   Max.   : 3.00595\nNow we can use the knn( ) function in the class R library to run the algorithm on the training data and then make predictions for each observation in the test data. The first argument calls for the \\(X\\)’s in the training data, the second calls for the \\(X\\)’s in the test data (for which we want to predict), the third calls for the \\(Y\\)’s in the training data, and the fourth calls for \\(k\\), the number of nearest neighbors we want to use to make the prediction.\nlibrary(class)\nknn1 = knn(stdtrainX, stdtestX, train$default, k=1)\nThe knn1 object now contains a vector of predicted \\(Y\\)’s for each value of \\(X\\) in the test data. We can then compare the predicted response \\(\\hat{Y}\\) to the true response in the test data \\(Y\\) to assess the performance of the classification algorithm. In particular, we will see the fraction of predictions the algorithm gets wrong.\nmean(knn1 != test$default)\n\n[1] 0.04466667\nIn this case, the 1-NN classifier as an error rate of about 4.5% (or equivalently, an accuracy of 95.5%).\nWe can try increasing \\(k\\) to see if there is any effect on predictive fit.\n# 5 nearest neighbors\nknn5 = knn(stdtrainX, stdtestX, train$default, k=5)\nmean(knn5 != test$default)\n\n[1] 0.029\n# 10 nearest neighbors\nknn10 = knn(stdtrainX, stdtestX, train$default, k=10)\nmean(knn10 != test$default)\n\n[1] 0.02633333\n# 50 nearest neighbors\nknn50 = knn(stdtrainX, stdtestX, train$default, k=50)\nmean(knn50 != test$default)\n\n[1] 0.024\n# 100 nearest neighbors\nknn100 = knn(stdtrainX, stdtestX, train$default, k=100)\nmean(knn100 != test$default) \n\n[1] 0.02733333\nWe would then likely choose the model that predicts best (i.e., has the lowest error/misclassification rate).\nThe last object of interest when doing classification is the confusion matrix, which allows us to decompose misclassification mistakes into two groups: false positives (predict \\(\\hat{Y}=1\\) when \\(Y=0\\)) and false negatives (predict \\(\\hat{Y}=0\\) when \\(Y=1\\)).\nLet’s produce the confusion matrix for the 10-NN classifier.\ntable(knn10,test$default)\n\n     \nknn10   No  Yes\n  No  2889   61\n  Yes   18   32\n# false positive rate\n18/(18+2889)\n\n[1] 0.00619195\n# false negative rate\n60/(60+33)\n\n[1] 0.6451613\nThe false negative rate is especially high, which would be concerning given the risks to the lending agency (e.g., bank).",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Classification</span>"
    ]
  },
  {
    "objectID": "07classification.html#logistic-regression",
    "href": "07classification.html#logistic-regression",
    "title": "7  Classification",
    "section": "7.2 Logistic Regression",
    "text": "7.2 Logistic Regression\nIssues with the \\(k\\)-NN algorithms include the fact they can’t accommodate categorical \\(X\\)’s, the algorithms aren’t based on a formal statistical model so we can’t do inference (or learn about how the \\(X\\)’s relate to \\(Y\\)), and there is an assumption that all \\(X\\)’s matter and matter equally in determining \\(Y\\).\nOur first solution to these problems is logistic regression.\nGiven a response \\(Y\\in\\{0,1\\}\\) and a set of predictors \\(X_1,\\dotsc,X_P\\), the logistic regression model is written as follows.\n\\[\\text{Pr}(Y=1|X)={\\exp(\\beta_0+\\beta_1X_1+\\dotsc+\\beta_pX_p)\\over 1 + \\exp(\\beta_0+\\beta_1X_1+\\dotsc+\\beta_pX_p)}\\]\nThe intuition for this formula is as follows. If \\(Y\\in\\{0,1\\}\\), then we can assume that \\(Y\\sim\\text{Bernoulli}(\\theta)\\) where \\(\\theta=\\text{Pr}(Y=1)\\). We can then write down a regression model for \\(\\theta\\) rather than \\(Y\\). The only remaining problem is that \\(\\theta\\in(0,1)\\), so we need to transform the linear regression function \\(h(X)=\\beta_0+\\beta_1X_1+\\dotsc+\\beta_pX_p)\\) in a way so that it is constrained to be between 0 and 1. The function \\(e^{h(X)}/(1 + e^{h(X)})\\) does just that.\nEstimating a logistic regression model in R can be done using the glm( ) function, which is similar to the lm( ) command we use to estimate linear regression models.\nLet’s illustrate with the training sample from the Default data set.\n\nglm.fit = glm(default ~ student + balance + income, family=\"binomial\", data=train)\nsummary(glm.fit)\n\n\nCall:\nglm(formula = default ~ student + balance + income, family = \"binomial\", \n    data = train)\n\nCoefficients:\n              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -1.101e+01  5.889e-01 -18.704   &lt;2e-16 ***\nstudentYes  -6.464e-01  2.846e-01  -2.271   0.0231 *  \nbalance      5.829e-03  2.781e-04  20.958   &lt;2e-16 ***\nincome       4.711e-06  9.875e-06   0.477   0.6333    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2090.7  on 6999  degrees of freedom\nResidual deviance: 1109.4  on 6996  degrees of freedom\nAIC: 1117.4\n\nNumber of Fisher Scoring iterations: 8\n\n\nNotice that we added one more option in the glm( ) function: type=\"binomial\". This option tells R to use the logistic regression model rather than other types of generalized linear models.\nThe output from the logistic regression model looks fairly similar to that of linear regression models. However, the interpretation of model paramters (and their estimates) changes a bit.\nFor example, we find that the coefficient on balance is estimated to be about 0.0058, which means that a one dollar increase in balance multiplies the odds of default by exp(0.0058)=1.006. Since this number is greater than 1, we can say that increasing the balance increases the odds of default.\nTo predict responses in the test data, we can use the predict( ) function in R. We again need to add one option: type=\"response\", which will tell R to return the predicted probabilities that \\(Y=1\\).\n\nglm.probs = predict(glm.fit, newdata=test, type=\"response\")\n\nThen we can compute \\(\\hat{Y}\\) by using the rule that \\(\\hat{Y}=\\text{Yes}\\) if the predicted probability is greater than 0.5 and \\(\\hat{Y}=\\text{No}\\) otherwise.\n\nglm.predict = ifelse(glm.probs&gt;0.5,\"Yes\",\"No\")\n\nJust as before, we can compare the model predictions with the actual \\(Y\\)’s in the test data to compute the out-of-sample error (misclassification) rate.\n\nmean(glm.predict != test$default)\n\n[1] 0.024\n\n\nThis error rate can be decomposed by producing the associated confusion matrix and computing the false positive and false negative rates.\n\ntable(glm.predict, test$default)\n\n           \nglm.predict   No  Yes\n        No  2896   61\n        Yes   11   32\n\n\n\n# false positive rate\n11/(11+2896)\n\n[1] 0.00378397\n\n\n\n# false negative rate\n61/(61+32)\n\n[1] 0.655914",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Classification</span>"
    ]
  },
  {
    "objectID": "07classification.html#classification-trees",
    "href": "07classification.html#classification-trees",
    "title": "7  Classification",
    "section": "7.3 Classification Trees",
    "text": "7.3 Classification Trees\nClassification trees offer the same advantages over logistic regression that regression trees do for linear regression. That is, classification trees provide a classification rule that does not assume any form of linearity in the covariates \\(X\\).\nThe nice thing is their implimentation in R is nearly identical to that of regression trees.\n\nlibrary(rpart)\n\n# estimate regression tree\ntree.fit = rpart(default ~ student + balance + income, method=\"class\", data=train)\n\n# plot estimated tree\nplot(tree.fit,uniform=TRUE,margin=0.05,main=\"DEFAULT\")\ntext(tree.fit)\n\n\n\n\n\n\n\n\nWe can again use the predict( ) function to predict the response values for the test data and compute the out-of-sample error (misclassification) rate. We need to specify the type=\"class\" option so that the predict( ) function returns the predicted values \\(\\hat{Y}\\).\n\ntree.predict = predict(tree.fit, newdata=test, type=\"class\")\nmean(tree.predict != test$default)\n\n[1] 0.027\n\n\nFinally, the error rate can be decomposed by producing the associated confusion matrix and computing the false positive and false negative rates.\n\ntable(tree.predict, test$default)\n\n            \ntree.predict   No  Yes\n         No  2880   54\n         Yes   27   39\n\n\n\n# false positive rate\n27/(27+2880)\n\n[1] 0.009287926\n\n\n\n# false negative rate\n54/(54+39)\n\n[1] 0.5806452",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Classification</span>"
    ]
  },
  {
    "objectID": "08clustering.html",
    "href": "08clustering.html",
    "title": "8  Clustering",
    "section": "",
    "text": "8.1 \\(k\\)-Means\nThe goal of clustering is to discover groups of similar observations among a set of variables \\(X_1,\\dotsc,X_p\\). Clustering is an example of an unsupervised learning method, as we only consider the features \\(X_1,\\dotsc,X_p\\) without an associated response \\(Y\\).\nTo illustrate clustering methods, we will use the Auto data in the ISLR R library. The data set contains information on the gas mileage, number of cylinders, displacement, horsepower, weight, acceleration, year, and origin for 392 vehicles.\nThe \\(k\\)-means clustering algorithm uses the variables \\(X_1,\\dotsc,X_p\\) to partition our observations \\(1,\\dotsc,n\\) into \\(k\\) non-overlapping groups. The partitioning is done based on the similarity of observations, where similarity is measured using Euclidean distance. Consequently, we will need to rescale our data.\nWe’ll isolate the first seven variables (mpg, cylinders, displacement, horsepower, weight, acceleration, year) and define them as \\(X\\).\n# select first seven columns of Auto data\nX = Auto[,1:7]\n\n# rescale X's\nstdX = scale(X)\n\n# set the name of each row to be the car name stored in the Auto data\nrownames(stdX) = Auto$name\n\n# summarize the rescaled data\nsummary(stdX)\n\n      mpg             cylinders        displacement       horsepower     \n Min.   :-1.85085   Min.   :-1.4492   Min.   :-1.2080   Min.   :-1.5190  \n 1st Qu.:-0.82587   1st Qu.:-0.8629   1st Qu.:-0.8544   1st Qu.:-0.7656  \n Median :-0.08916   Median :-0.8629   Median :-0.4149   Median :-0.2850  \n Mean   : 0.00000   Mean   : 0.0000   Mean   : 0.0000   Mean   : 0.0000  \n 3rd Qu.: 0.71160   3rd Qu.: 1.4821   3rd Qu.: 0.7773   3rd Qu.: 0.5594  \n Max.   : 2.96657   Max.   : 1.4821   Max.   : 2.4902   Max.   : 3.2613  \n     weight         acceleration           year         \n Min.   :-1.6065   Min.   :-2.73349   Min.   :-1.62324  \n 1st Qu.:-0.8857   1st Qu.:-0.64024   1st Qu.:-0.80885  \n Median :-0.2049   Median :-0.01498   Median : 0.00554  \n Mean   : 0.0000   Mean   : 0.00000   Mean   : 0.00000  \n 3rd Qu.: 0.7501   3rd Qu.: 0.53778   3rd Qu.: 0.81993  \n Max.   : 2.5458   Max.   : 3.35597   Max.   : 1.63432\nLet’s start by runing the \\(k\\)-means algorithm with \\(k=2\\) and only using the mpg and horsepower variables.\n# estimate clusters\nkm2 = kmeans(stdX[,c(1,4)],2)\n\n# plot clusters\nplot(mpg, horsepower, col=km2$cluster)\nWe can see how the algorithm divides the observations into two groups: the red observations have a high mpg but lower horsepower, while the black observations have a low mpg but high horsepower.\nNow let’s try using all seven variables to define the clusters.\n# estimate clusters\nkm2 = kmeans(stdX,2)\n\n# plot clusters pver mpg and horsepower\nplot(mpg, horsepower, col=km2$cluster)\nThe plot looks similar: even when we use all variables, the first group of cars (black observations) have a low mpg and high horsepower while the second group (red observations) have a high mpg and low horsepower.\nThe plot above only shows the clustering solution with respect to two variables (mpg and horsepower). To examine how the clusters are defined over all variables, we can use the pairs( ) function.\n# plot clusters over all variables\npairs(stdX, col=km2$cluster, xaxt=\"n\", yaxt=\"n\")\nLastly, we can explore clustering solutions for different values of \\(k\\). For simplicity, we will only examine the clusters for the mpg and horsepower variables.\n# estimate clusters\nkm3 = kmeans(stdX,3)\nkm4 = kmeans(stdX,4)\nkm5 = kmeans(stdX,5)\n\n# plot clusters over mpg and horsepower\npar(mfrow=c(2,2), mar=c(4.1,4.1,2.1,2.1))\nplot(mpg, horsepower, col=km2$cluster)\nplot(mpg, horsepower, col=km3$cluster)\nplot(mpg, horsepower, col=km4$cluster)\nplot(mpg, horsepower, col=km5$cluster)\nAs \\(k\\) increases, we get a more granular picture of car segments. However, the problem of interpretation becomes more difficult: What is it that actually differentiates these clusters from each other? We are only plotting the data over two variables, but the other variables also contribute in the determination of cluster assignments.\nMoreover, how should we determine an appropriate value of \\(k\\)? Hierachical clustering provides a partial solution.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Clustering</span>"
    ]
  },
  {
    "objectID": "08clustering.html#hierarchical-clustering",
    "href": "08clustering.html#hierarchical-clustering",
    "title": "8  Clustering",
    "section": "8.2 Hierarchical Clustering",
    "text": "8.2 Hierarchical Clustering\nHierarchical clustering addresses the issue of having to choose the number of clusters \\(k\\), and instead considers a sequence of clusters from \\(k = 1,\\dotsc,n\\). We’ll use the hclust( ) function and dendextend package to fit and plot the output from hierarchical clustering models.\n\nlibrary(dendextend)\n\n\n# estimate clusters\nhc = hclust(dist(X))\n\n# plot clusters\ndend = as.dendrogram(hc)\nlabels_cex(dend) = .25\nplot(dend)\n\n\n\n\n\n\n\n\nBecause we have a few hundred observations, the plot – which called a “dendrogram” – becomes difficult to read and interpret on a small scale (meaning we would need a much larger plotting window!).\nSuppose we were interested in a two group clustering solution. Then we can simply color the dendrogram based on the first split.\n\ndend = as.dendrogram(hc)\ndend = color_labels(dend,k=2)\ndend = color_branches(dend,k=2)\nlabels_cex(dend) = .25\nplot(dend)\n\n\n\n\n\n\n\n\nWe can do the same for a larger number of groups, too.\n\ndend = as.dendrogram(hc)\ndend = color_labels(dend,k=10)\ndend = color_branches(dend,k=10)\nlabels_cex(dend) = .25\nplot(dend)\n\n\n\n\n\n\n\n\nNotice that when interpreting a dendrogram, the analyst must still “choose” \\(k\\), so the problem still hasn’t really gone away. The only benefit with hierarchical clustering methods is that the analyst can quickly and easily examine the landscape of clustering solutions to understand how the value of \\(k\\) impacts different clustering solutions.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Clustering</span>"
    ]
  }
]